---
layout: post
title:  "Simple Hardware-Efficient Long Convolutions for Sequence Modeling"
date:   2023-02-03 22:21:59 +00:00
image: /images/header.png
categories: research
author: "Elliot Epstein"
venue: "ICML"
authors: "<strong>Elliot L. Epstein*</strong>, Dan Y. Fu*, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, Christopher RÃ© "
subtitle: "Simple Hardware-Efficient Long Convolutions for Sequence Modeling"
arxiv: https://arxiv.org/abs/2302.06646
code: https://github.com/Elliotepsteino/safari
blog_post: https://hazyresearch.stanford.edu/blog/2023-02-15-long-convs
---

What is the simplest architecture you can use to get good performance on sequence modeling with subquadratic compute scaling in the sequence length? State space models (SSMs) have high performance on long sequence modeling but require sophisticated initialization techniques and specialized implementations for high quality and runtime performance. We study whether directly learning long convolutions over the sequence can can match SSMs in performance and efficiency. 
